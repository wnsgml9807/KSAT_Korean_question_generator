---

### 4️⃣ 세부 기술 구현

#### 프로젝트 구조

```
KSAT Agent/
├── frontend/               # Streamlit 기반 웹 인터페이스 (Git 관리, Streamlit Cloud 배포)
│   ├── pages/
│   │   └── about.py        # 프로젝트 소개 페이지
│   ├── app_main.py         # 메인 앱 진입점
│   ├── requirements.txt    # Frontend 의존성 (Streamlit Cloud용)
│   └── .streamlit/         # Streamlit 설정 (필요시)
│
├── backend/                # 멀티 에이전트 시스템 코어 (Git 관리)
│   ├── DB/                 # 데이터베이스 (ChromaDB, SQLite)
│   ├── agent_server.py     # FastAPI 서버 진입점
│   ├── graph_factory.py    # LangGraph 워크플로우
│   ├── agents_prompt/      # 에이전트별 프롬프트
│   ├── tools.py            # 에이전트 공용 도구
│   ├── handoff_tools.py    # 에이전트 핸드오프 도구
│   ├── model_config.py     # LLM 모델 설정
│   ├── Dockerfile          # Backend Docker 이미지 빌드용
│   ├── docker-compose.yaml # Docker Compose 설정
│   ├── supervisord.conf    # Supervisor 설정
│   └── requirements.txt    # Backend 의존성
│  
└── Parser/                 # (참고: 기출 문제 분석 및 데이터 전처리 로직 - Git 관리 범위 외)

```

#### 주요 기술 스택

| 영역 | 기술 | 용도 |
|------|------|------|
| **프론트엔드** | Streamlit | 사용자 인터페이스 및 실시간 진행상황 표시 |
| **백엔드** | FastAPI | API 서버 및 스트리밍 응답 처리 |
| **멀티에이전트** | LangGraph | 에이전트 간 상태 관리 및 워크플로우 |
| | LangChain | 메모리, 체인, 도구 활용 |
| **데이터베이스** | ChromaDB | 의미 검색 및 벡터 저장소 |
| | SQLite | 세션 상태 관리 및 체크포인트 |
| **LLM** | OpenAI GPT-4.1 | Supervisor 에이전트 |
| | Anthropic Claude-3.7 | Passage Editor 에이전트 |
| | Gemini-2.5 Flash | Researcher, Question Editor, Validator 에이전트 |
| **임베딩** | OpenAI text-embedding-3 | 한국어 텍스트 벡터화 |
| **배포** | Docker | 애플리케이션 컨테이너화 |
| | GCP (Google Cloud Platform) | 클라우드 VM 및 인프라 |

#### LangGraph 선택 배경

수능 지문 생성은 여러 단계의 복잡한 작업이 순차적으로 이루어져야 합니다. 초기에는 단일 LLM을 사용한 접근법도 고려했으나, 다음과 같은 한계에 직면했습니다:
```
1. 토큰 제한에 빠르게 도달 - 수능 지문 생성에 필요한 모든 지시와 컨텍스트가 16K 토큰을 쉽게 초과
2. 역할 혼란 - 하나의 모델이 연구자, 작가, 검토자 역할을 번갈아 수행하며 일관성 상실
3. 전문성 결여 - 전체 태스크를 한 번에 처리하려다 보니 각 단계별 품질이 저하
```
LangGraph는 이러한 문제들을 해결할 수 있는 최적의 프레임워크였습니다:

> **State 관리의 효율성** : LangChain의 단순 체인과 달리, LangGraph는 복잡한 상태를 그래프 노드 간에 유지하고 전달할 수 있어 여러 에이전트가 협업하기에 이상적입니다.
>
> **유연한 워크플로우** : 조건부 경로와 순환 가능한 그래프 구조를 통해 "검증 실패 → 재작업" 같은 복잡한 워크플로우를 자연스럽게 구현할 수 있습니다.
>
> **체크포인팅** : 내장된 체크포인트 기능을 내장하여, 장시간 실행 과정에서 발생할 수 있는 오류나 중단으로부터 상대적으로 안전합니다.

```python
# graph_factory.py 중 일부
builder = StateGraph(MultiAgentState)

builder.add_node("researcher", researcher_agent)
builder.add_node("architecture", architecture_agent)
...
builder.add_conditional_edges(
    "validator",
    lambda s: ("END" if s["validation_ok"] else "architecture")
)
graph = builder.compile()
```

#### LLM 모델 선택

각 태스크의 특성에 맞게 최적화된 LLM 모델을 사용했습니다.

실제 테스트 결과, Claude는 문체와 깊이 있는 내용 생성에 강점을 보였고, Gemini는 분석과 문항 생성의 속도에서 우위를 보였습니다. 

이러한 모델 조합을 통해 품질과 비용 효율성 모두를 확보할 수 있었습니다.

| 에이전트 | 사용 모델 | 선택 이유 |
|---------|---------|----------|
| **Supervisor** | ```GPT-4.1``` | • 지시사항을 정확히 이해하고 따르는 능력(instruction following)이 탁월함<br>• 사용자 친화적인 어조로 소통하며 작업 흐름을 조율<br>• 복잡한 작업 흐름을 관리하는 능력이 뛰어남 |
| **Passage Editor** | ```Claude-3.7``` | • 한국어 문체가 자연스럽고 수능 지문 스타일의 고품질 텍스트 생성<br>• 복잡한 개념도 논리적 구조를 유지하며 1,700자 내외로 압축하는 능력 탁월<br>• 내적 일관성이 높은 지문 작성 가능 |
| **Question Editor & Validator** | ```Gemini-2.5 Flash``` | • 빠른 응답 속도로 지문을 분석하고 문항 생성<br>• 수학적/논리적 관계 파악에 강점, 선지 간 변별력 체크에 효과적<br>• 비용 효율적이라 반복적 검증 과정에 적합 |
  


```python
passage_editor_agent = create_react_agent(
    model=Model_anthropic_3_7,
    tools=passage_editor_tools,
    prompt=passage_prompt  # 세부 지침 포함
)
```

#### 에이전트 간 작업 통제권 전환 메커니즘

초기 버전에서는 에이전트 간 전환을 위해 LangGraph의 조건부 엣지를 이용했습니다. 그러나 이는 다음과 같은 문제를 야기했습니다:

```
1. 확장성 한계 - 새 에이전트 추가 시, 분기 로직과 코드를 일일이 수정해야 함
2. 복잡한 상태 관리 - 누가 다음에 실행될지 추적하는 로직이 복잡해짐
3. 유연성 부족 - 실행 중 워크플로우 변경이 어려움
```

이를 해결하기 위해 Command 객체를 활용한 핸드오프 도구를 활용했습니다:

```python
@tool
async def handoff_for_agent(
    agent_name: Literal[
        "researcher","architecture","passage_editor",
        "question_editor","validator","end"
    ],
    state: Annotated[dict, InjectedState],
    tool_call_id: InjectedToolCallId,
):
    """다음 단계로 통제권을 넘깁니다."""
    message = ToolMessage(
        content="통제권이 넘어왔습니다. 지금까지의 내용을 확인하고 당신의 역할을 수행하세요.",
        tool_call_id=tool_call_id,
    )
    return Command(
        goto=("END" if agent_name=="end" else agent_name),
        graph=Command.PARENT,
        update={"messages": state["messages"] + [message]},
    )
```

이 접근법의 가장 큰 장점은 간단한 프롬프트 수정만으로 워크플로우 변경이 가능하다는 점입니다.

코드 한 줄 수정 없이, Supervisor의 프롬프트만 변경하면 새로운 워크플로우가 적용됩니다.

#### ChromaDB 선택 이유와 임베딩 전략

LLM이 기출 문제를 능동적으로 검색할 수 있는 시스템을 구축하기 위해 여러 벡터 데이터베이스 옵션을 검토했습니다:

> 검토한 벡터 DB 후보:
> - Pinecone: 관리형 서비스, 확장성 우수
> - Milvus: 고성능 분산 처리 가능
> - ChromaDB: 경량화, 로컬 실행 가능, Python 통합 우수


최종적으로 ChromaDB를 선택한 이유는 다음과 같습니다:

1. **로컬 클라이언트**: 서버리스 환경에서도 SQLite 백엔드로 완전히 작동하여 외부 의존성 최소화

2. **메타데이터 필터링**: 연도, 월, 영역 등의 복합 필터링을 where 절로 간단히 구현 가능

3. **임베딩 유연성**: 다양한 임베딩 모델을 쉽게 교체할 수 있는 아키텍처 지원

한국어 텍스트 임베딩을 위해 여러 모델을 벤치마크한 결과, OpenAI의 text-embedding-3-large가 한글 수능 지문의 의미적 유사성을 가장 정확히 포착하는 것으로 확인되어 채택했습니다.

```python
results = collection.query(
    query_texts=[query],
    n_results=n_results,
    where={"year": year, "field": field} or None
)
```

#### 토큰 비용 관리

AI 모델의 컨텍스트 제한과 비용 문제를 해결하기 위해, 일반적으로 사용되는 요약 모델 접근법 대신 독창적인 "절삭+주입" 전략을 개발했습니다:

```python
# 비용 효율적인 토큰 관리 전략
async def message_truncation_hook(state: MultiAgentState) -> dict:
    """메시지가 너무 길 경우 앞뒤 일부만 유지하는 훅"""
    processed_messages = []
    
    for msg in state.get('messages', []):
        # 시스템 메시지나 사용자 메시지는 항상 보존
        if isinstance(msg, (SystemMessage, HumanMessage)):
            processed_messages.append(msg)
            continue
            
        # AI 메시지나 도구 메시지는 길이 체크
        content = get_content(msg)
        if content and num_tokens(content) > TRUNCATE_THRESHOLD:
            # 앞부분 500자 + ... + 뒷부분 500자 형태로 절삭
            truncated = content[:500] + "\n...[중략: 약 " + str(num_tokens(content) - 1000) + "토큰 분량의 내용이 생략됨]...\n" + content[-500:]
            
            # 복사본 생성 후 내용 교체
            msg_copy = deepcopy(msg)
            set_content(msg_copy, truncated)
            processed_messages.append(msg_copy)
        else:
            processed_messages.append(msg)
    
    return {"llm_input_messages": processed_messages}
```

이 접근법을 통해 얻은 이점:

1. **요약 모델 비용 절감**: 추가 LLM 호출 없이 단순 절삭으로 70-80%의 토큰을 절약

2. **중요 정보 보존**: 일반적으로 메시지의 앞부분과 뒷부분에 중요 정보가 집중되어 있으므로, 이 부분을 유지하는 것만으로도 맥락 이해에 충분

3. **처리 지연 최소화**: 요약 모델 호출 대비 약 2-3초의 시간 절약

실제 테스트에서 품질 저하 없이 토큰 사용량을 약 65% 줄일 수 있었습니다.

---

### 5️⃣ 개념 지도

>개념 지도에 대한 상세한 설명은 부록을 참고 부탁드립니다.

#### 개념 지도 소개

교육 콘텐츠 평가에서 "논리 구조가 복잡하다"나 "정보 밀도가 높다"와 같은 표현은 주관적이고 정성적인 평가에 불과했습니다. 이로 인해 난이도와 정보 밀도의 조절이 출제자의 경험과 직관에 의존해야 했고, AI를 활용한 자동화 시스템 구축이 어려웠습니다.

KSAT Agent는 자체 개발한 **개념 지도**를 통해 이러한 정성적 평가를 정량화했습니다:

| 기능 | 기존 방식 | 개념 지도의 효과 |
|------|-----------|--------------------------------|
| 논리 구조 파악 | 전문가의 주관적 평가 | **엣지 타입·깊이**를 계량화 |
| 정보 밀도 판단 | 단순 글자수·문단수 | **노드 수 / 1천 token** 지표 |
| 선지 생성 근거 | 수작업 검색 | 그래프 경로 탐색 |

##### ① 예시 지문

> "촉매는 화학 반응의 활성화 에너지를 낮춰 반응 속도를 높입니다. 그러나 촉매 자체는 반응 전후에 변하지 않습니다."

##### ② 자동 추출된 그래프

| 노드 ID | 라벨 | 설명 |
|---------|------|------|
| N1 | Catalyst | 촉매 |
| N2 | ActivationEnergy | 활성화 에너지 |
| N3 | ReactionRate | 반응 속도 |

| 엣지 | 타입·레이블 | 의미 |
|------|-------------|------|
| N1→N2 | Causality · `influences` | 촉매가 에너지를 낮춤 |
| N2→N3 | Causality · `causes` | 낮아진 에너지가 속도 증가 초래 |
| N1→N1 | QuantComparison · `is_equal_to` | 촉매 전후 동일(자기 보존) |

이러한 정량적 지표를 통해 Question Editor는 "원인→결과→불변" 경로를 선지 패턴으로 활용할 수 있습니다.

이처럼 개념 지도는 논리 구조, 정보 밀도, 추론 경로를 객관적 수치로 변환하여, 

개념 지도의 적용을 통해 에이전트에게 지문의 난이도와 정보 밀도를 정량적으로 주문할 수 있게 됩니다.

--- 

### 6️⃣ 지문 작성용 LLM 파인튜닝


**Problem** 
> "본질적으로 '쉽고 직관적으로' 설명하도록 학습된 LLM 모델의 특성 상,<br>
> 프롬프트만으로 정보의 밀도와 독해 난이도를 극적으로 향상시키기에는 한계가 있다고 느꼈다."

따라서 다음의 데이터 파이프라인을 구축하여 소규모 파인튜닝을 진행한 결과, 지문 퀄리티를 극적으로 향상시킬 수 있었다.

| 구분 | 내용 |
|------|------|
| 학습 데이터 1 | LLM을 활용해 **기출 지문**에서 추출한 개념 지도 |
| 학습 데이터 2 | 기출 지문 원문 |

#### ① SFT 데이터셋 생성 파이프라인

| 단계               | 스크립트                              | 핵심 로직·함수                                                   | 산출물                                     |
| ---------------- | --------------------------------- | ---------------------------------------------------------- | --------------------------------------- |
| **1. 기출 txt 파싱** | `concept_map_converter.py`        | `passage_parser()` 〈줄바꿈·특수 기호 정리〉                          | `passage` (*str*)                       |
| **2. 개념 지도 변환**  | `convert_concept_map_async()`     | Gemini-2.5-flash -> JSON (15 ± 5 노드)                       | `concept_map.json`                      |
| **3. JSONL 빌드**  | `training_data_generator_async()` | 3-turn 메시지 샘플<br>`system→user(concept)→assistant(passage)` | `…training_dataset.jsonl`               |
| **4. 검증·Split**  | `data_validation_cli.py`          | 토큰·포맷 검사 → 90 / 10 split                                   | `train.jsonl` (76) <br>`val.jsonl` (10) |

> *데이터 통계* : 전체 ≈ 12 만 tokens, 1 샘플 평균 1 500 tokens

---

#### ② Fine-Tune 설정 (OpenAI)

